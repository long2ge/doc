

自增锁
在对自增列进行操作时，存在着自增锁，mysql的innodb_autoinc_lock_mode参数控制着自增锁的上锁机制。该参数有0、1、2三种模式：
0：语句执行结束后释放自增锁，MySQL5.0时采用这种模式，并发度较低。
1：mysql的默认设置。普通的insert语句申请后立马释放，insert select、replace insert、load data等批量插入语句要等语句执行结束后才释放，并发读得到提升
2：所有的语句都是申请后立马释放，并发度大大提升！但是在binlog为statement格式时，主从数据会发生不一致。这一块网上有很多例子，我不做介绍了。

大家生产上该参数设为2，然后binlog设为row格式就行



可以看到，批量插入，导致下一个id值不为9了，再插入数据，即产生了空洞，这里是由mysql申请自增值的机制所造成的，
MySQL在批量插入时，若一个值申请一个id，效率太慢，影响了批量插入的速度，故mysql采用下面的策略批量申请id。

语句执行过程中，第一次申请自增id，会分配1个；
1个用完以后，这个语句第二次申请自增id，会分配2个；
2个用完以后，还是这个语句，第三次申请自增id，会分配4个；
依此类推，同一个语句去申请自增id，每次申请到的自增id个数都是上一次的两倍。
下表表示了上面实验演示的id分配过程。

    	
    INSERT INTO `test_a`(`name`) VALUES ('a'), ('a'), ('a'), ('a');
    
    insert into test_a(name) select name from test_a
    
    INSERT INTO `test_a`(`name`) VALUES ('a'), ('a'), ('a'), ('a');
    
     select Auto_increment from information_schema.tables where table_name='test_a';

造成自增id不连续的情况可能有：

1.唯一键冲突
2.事务回滚
3.insert ... select语句批量申请自增id


Delete、update：删除中间数据，会造成空动，而修改自增id值，也会造成空洞（这个很少）。
Insert：插入报错（唯一键冲突与事务回滚），会造成空洞，因为这时候自增id已经分配出去了，新的自增值已经生成，


### 我们先来看下MySQL 对自增值的保存策略：

    InnoDB 引擎的自增值，其实是保存在了内存里，并且到了 MySQL 8.0 版本后，才有了“自增值持久化”的能力，也就是才实现了“如果发生重启，表的自增值可以恢复为 MySQL 重启前的值”，具体情况是：
    在 MySQL 5.7 及之前的版本，自增值保存在内存里，并没有持久化。每次重启后，第一次打开表的时候，都会去找自增值的最大值 max(id)，然后将 max(id)+1 作为这个表当前的自增值。
    举例来说，如果一个表当前数据行里最大的 id 是 10，AUTO_INCREMENT=11。这时候，我们删除 id=10 的行，AUTO_INCREMENT 还是 11。但如果马上重启实例，重启后这个表的 AUTO_INCREMENT 就会变成 10。
    也就是说，MySQL 重启可能会修改一个表的 AUTO_INCREMENT 的值。
    在 MySQL 8.0 版本，将自增值的变更记录在了 redo log 中，重启的时候依靠 redo log 恢复重启之前的值。



### 四、自增锁的优化
自增id锁并不是一个事务锁，而是每次申请完就马上释放，以便允许别的事务再申请

但在MySQL5.0版本的时候，自增锁的范围是语句级别。也就是说，如果一个语句申请了一个表自增锁，这个锁会等语句执行结束以后才释放

MySQL5.1.22版本引入了一个新策略，新增参数innodb_autoinc_lock_mode，默认值是1

1.这个参数设置为0，表示采用之前MySQL5.0版本的策略，即语句执行结束后才释放锁

2.这个参数设置为1

普通insert语句，自增锁在申请之后就马上释放
类似insert … select这样的批量插入数据的语句，自增锁还是要等语句结束后才被释放
3.这个参数设置为2，所有的申请自增主键的动作都是申请后就释放锁

为了数据的一致性，默认设置为1



### 批量插入数据

1）让原库的批量插入数据语句，固定生成连续的id值。所以，自增锁直到语句执行结束才释放，就是为了达到这个目的

2）在binlog里面把插入数据的操作都如实记录进来，到备库执行的时候，不再依赖于自增主键去生成。也就是把innodb_autoinc_lock_mode设置为2，同时binlog_format设置为row

如果有批量插入数据（insert … select、replace … select和load data）的场景时，从并发插入数据性能的角度考虑，建议把innodb_autoinc_lock_mode设置为2，同时binlog_format设置为row，这样做既能并发性，又不会出现数据一致性的问题




InnoDB存储引擎既支持行级锁（row-level locking），也支持表级锁，但默认情况下是采用行级锁。

表级锁：开销小，加锁快；不会出现死锁；锁定粒度大，发生锁冲突的概率最高,并发度最低。
行级锁：开销大，加锁慢；会出现死锁；锁定粒度最小，发生锁冲突的概率最低,并发度也最高。

innodb 行级锁 record-level
lock大致有三种：record lock, gap lock and Next-KeyLocks。

record lock 锁住某一行记录
gap lock 锁住某一段范围中的记录
next key lock 是前两者效果的叠加。

nnoDB实现了以下两种类型的行锁：

共享锁：允许一个事务去读一行，阻止其他事务获得相同数据集的排他锁；
排他锁：允许获得排他锁的事务更新数据，阻止其他事务取得相同数据集的共享读锁和排他写锁。







因为索引扫描书超过30%时，会进行全表扫描。


InnoDB的行锁实现的特点：只有通过索引条件检索数据，InnoDB才会使用行级锁，否则，InnoDB将会使用表锁。因为MySQL的行锁是针对索引加的锁，

InnoDB使用间隙锁的目的：一是为了防止幻读，二是为了满足其恢复和复制的需要。



MySQL InnoDB存储引擎，实现的是基于多版本的并发控制协议——MVCC (Multi-Version Concurrency Control) (注：与MVCC相对的，
是基于锁的并发控制，Lock-Based Concurrency Control)。MVCC最大的好处，相信也是耳熟能详：读不加锁，读写不冲突。



在MVCC并发控制中，读操作可以分成两类：快照读 (snapshot read)与当前读 (current read)。

快照读：简单的select操作，属于快照读，不加锁。
 当前读：特殊的读操作，插入/更新/删除操作，属于当前读，需要加锁。
 
 
 
     脏读:当一个事务进行的操作还未提交时，另外一个事务读到了修改的数据，这就是脏读，但是RR级别事务避免了脏读。  
     
     不可重复读:是指在一个事务内，多次读同一数据。在这个事务还没有结束时，另外一个事务也访问该同一数据。
     那么，在第一个事务中的两次读数据之间，由于第二个事务的修改，那么第一个事务两次读到的的数据可能是不一样的。
     这样就发生了在一个事务内两次读到的数据是不一样的，因此称为是不可重复读。
     但是，RR级别是不会出现不一样的结果的，即使另一个事务提交了修改他也查不到变化。  
       
     幻读：第一个事务对一个表中的数据进行了修改，这种修改涉及到表中的全部数据行。同时，第二个事务也修改这个表中的数据，  
     
     
在InnoDB事务中，行锁是在需要的时候才加上的，但并不是不需要了就立刻释放，而是要等到事务结束时才释放。这个就是两阶段锁协议。
如果你的事务中需要锁多个行，要把最可能造成锁冲突、最可能影响并发度的锁尽量往后放。


死锁和死锁检测

当并发系统中不同线程出现循环资源依赖，涉及的线程都在等待别的线程释放资源时，就会导致这几个线程都进入无限等待的状态，称为死锁。  

减少死锁的主要方向，就是控制访问相同资源的并发事务量。



更新数据都是先读后写的，而这个读，只能读当前的值，称为“当前读”（current read）。


事务的可重复读的能力是怎么实现的？

可重复读的核心就是一致性读（consistent read）；而事务更新数据的时候，只能用当前读。如果当前的记录的行锁被其他事务占用的话，就需要进入锁等待。



### 间隙锁(Gap Lock)。

间隙锁的引入，可能会导致同样的语句锁住更大的范围，这其实是影响了并发度的。